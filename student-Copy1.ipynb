{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Angie Schmitz\n",
    "* Student pace: self paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll clean, explore, and model this dataset with a multivariate linear regression to predict the sale price of houses as accurately as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be four deliverables for this project:\n",
    "\n",
    "A well documented Jupyter Notebook containing any code you've written for this project and comments explaining it. This work will need to be pushed to your GitHub repository in order to submit your project.\n",
    "\n",
    "A short Keynote/PowerPoint/Google Slides presentation (delivered as a PDF export) giving a high-level overview of your methodology and recommendations for non-technical stakeholders. Make sure to also add and commit this pdf of your non-technical presentation to your repository with a file name of presentation.pdf.\n",
    "\n",
    "A blog post (800-1500 words) about one element of the project - it could be the EDA, the feature selection, the choice of visualizations or anything else technical relating to the project. It should be targeted at your peers - aspiring data scientists.\n",
    "\n",
    "A Video Walkthrough of your non-technical presentation. Some common video recording tools used are Zoom, Quicktime, and Nimbus. After you record your presentation, publish it on a service like YouTube or Google Drive, you will need a link to the video to submit your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Getting Started\n",
    "Please start by reviewing this document. If you have any questions, please ask them in Slack ASAP so (a) we can answer the questions and (b) so we can update this repository to make it clearer.\n",
    "\n",
    "Be sure to let the instructor team know when you’ve started working on a project, either by reaching out over Slack or, if you are in a full-time or part-time cohort, by connecting with your Cohort Lead in your weekly 1:1. If you’re not sure who to reach out to, post in the #online-ds-sp-000 channel in Slack.\n",
    "\n",
    "Once you're done with the first 12 sections, please start on the project. Do that by forking this repository, cloning it locally, and working in the student.ipynb file. Make sure to also add and commit a pdf of your presentation to the repository with a file name of presentation.pdf.\n",
    "\n",
    "2. The Project Review\n",
    "What to expect from the Project Review\n",
    "Project reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation (sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice).\n",
    "\n",
    "The project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\n",
    "\n",
    "1. Deliver your PDF presentation to a non-technical stakeholder.\n",
    "In this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation should not exceed 5 minutes, giving the \"stakeholder\" 5 minutes to ask questions.\n",
    "\n",
    "In the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\n",
    "\n",
    "2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\n",
    "* \"how did you pick the question(s) that you did?\"\n",
    "* \"why are these questions important from a business perspective?\"\n",
    "* \"how did you decide on the data cleaning options you performed?\"\n",
    "* \"why did you choose a given method or library?\"\n",
    "* \"why did you select those visualizations and what did you learn from each of them?\"\n",
    "* \"why did you pick those features as predictors?\"\n",
    "* \"how would you interpret the results?\"\n",
    "* \"how confident are you in the predictive quality of the results?\"\n",
    "* \"what are some of the things that could cause the results to be wrong?\"\n",
    "\n",
    "Think of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don't be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\n",
    "\n",
    "If any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following:\n",
    "\n",
    "Perform additional data cleanup, visualization, feature selection, modeling and/or model validation\n",
    "Submit an improved version\n",
    "Meet again for another Project Review\n",
    "\n",
    "\n",
    "Requirements\n",
    "This section outlines the rubric we'll use to evaluate your project.\n",
    "\n",
    "1. Technical Report Must-Haves\n",
    "For this project, your Jupyter Notebook should meet the following specifications:\n",
    "\n",
    "Organization/Code Cleanliness\n",
    "The notebook should be well organized, easy to follow, and code should be commented where appropriate.\n",
    "Level Up: The notebook contains well-formatted, professional looking markdown cells explaining any substantial code. All functions have docstrings that act as professional-quality documentation.\n",
    "The notebook is written for a technical audience with a way to both understand your approach and reproduce your results. The target audience for this deliverable is other data scientists looking to validate your findings.\n",
    "\n",
    "Visualizations & EDA\n",
    "Your project contains at least 4 meaningful data visualizations, with corresponding interpretations. All visualizations are well labeled with axes labels, a title, and a legend (when appropriate).\n",
    "You pose at least 3 meaningful questions and aswer them through EDA. These questions should be well labled and easy to identify inside the notebook.\n",
    "Level Up: Each question is clearly answered with a visualization that makes the answer easy to understand.\n",
    "Your notebook should contain 1 - 2 paragraphs briefly explaining your approach to this project through the OSEMN framework.\n",
    "\n",
    "Model Quality/Approach\n",
    "Your model should not include any predictors with p-values greater than .05.\n",
    "Your notebook shows an iterative approach to modeling, and details the parameters and results of the model at each iteration.\n",
    "\n",
    "Level Up: Whenever necessary, you briefly explain the changes made from one iteration to the next, and why you made these choices.\n",
    "You provide at least 1 paragraph explaining your final model.\n",
    "You pick at least 3 coefficients from your final model and explain their impact on the price of a house in this dataset.\n",
    "\n",
    "2. Non-Technical Presentation Must-Haves\n",
    "The second deliverable should be a Keynote, PowerPoint or Google Slides presentation delivered as a pdf file in your fork of this repository with the file name of presentation.pdf detailing the results of your project. Your target audience is non-technical people interested in using your findings to maximize their profit when selling their home.\n",
    "\n",
    "Your presentation should:\n",
    "\n",
    "Contain between 5 - 10 professional-quality slides.\n",
    "Level Up: The slides should use visualizations whenever possible, and avoid walls of text.\n",
    "Take no more than 5 minutes to present.\n",
    "Avoid technical jargon and explain the results in a clear, actionable way for non-technical audiences.\n",
    "Based on the results of your models, your presentation should discuss at least two concrete features that highly influence housing prices.\n",
    "\n",
    "3. Blog Post\n",
    "Please also write a blog post about one element of the project - it could be the EDA, the feature selection, the choice of visualizations or anything else technical relating to the project. It should be between 800-1500 words and should be targeted at your peers - aspiring data scientists.\n",
    "\n",
    "Submitting your Project\n",
    "You’re almost done! In order to submit your project for review, include the following links to your work in the corresponding fields on the right-hand side of Learn.\n",
    "\n",
    "GitHub Repo: Now that you’ve completed your project in Jupyter Notebooks, push your work to GitHub and paste that link to the right. (If you need help doing so, review the resources here.) Reminder: Make sure to also add and commit a pdf of your non-technical presentation to the repository with a file name of presentation.pdf.\n",
    "Blog Post: Include a link to your blog post.\n",
    "Record Walkthrough: Include a link to your video walkthrough.\n",
    "Hit \"I'm done\" to wrap it up. You will receive an email in order to schedule your review with your instructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Names and descriptions for Kings County Data Set\n",
    "* **id** - unique identified for a house\n",
    "* **dateDate** - house was sold\n",
    "* **pricePrice** -  is prediction target\n",
    "* **bedroomsNumber** -  of Bedrooms/House\n",
    "* **bathroomsNumber** -  of bathrooms/bedrooms\n",
    "* **sqft_livingsquare** -  footage of the home\n",
    "* **sqft_lotsquare** -  footage of the lot\n",
    "* **floorsTotal** -  floors (levels) in house\n",
    "* **waterfront** - House which has a view to a waterfront\n",
    "* **view** - Has been viewed\n",
    "* **condition** - How good the condition is ( Overall )\n",
    "* **grade** - overall grade given to the housing unit, based on King County grading system\n",
    "* **sqft_above** - square footage of house apart from basement\n",
    "* **sqft_basement** - square footage of the basement\n",
    "* **yr_built** - Built Year\n",
    "* **yr_renovated** - Year when house was renovated\n",
    "* **zipcode** - zip\n",
    "* **lat** - Latitude coordinate\n",
    "* **long** - Longitude coordinate\n",
    "* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSEMN (Obtain, Scrub, Explore, Model, Interpret) framework\n",
    "\n",
    "Obtain: The goal of this project is to accurately predict home sales price in King County, WA by building a multivariate linear regression model. As such, home sales price and information related to the home (such as square footage, location, condition, etc.) was provided by the instructors.\n",
    "\n",
    "Scrub: In this section, we replace missing values in waterfront, view and keep missingness in yr_renovated by categorizing it in it's own separate category. Next we convert mismatched datatypes (integers to strings and vice versa) to their appropriate formats. We deal with multicollinearity and encoding dummy variables after explorating the data.\n",
    "\n",
    "Explore: Next, we ask some basic questions about how location, square footage and time affects price. First, how does location affect price? We answer this by layering a scatterplot on top of a map to visualize how prices vary geographically. We also explore which zipcodes are the most expensive and which zipcodes are the most economical. Next, we want to know how square footage affects price. To answer this, we generate a grid of scatterplots visualizing the relationship between price and square footage. Finally, we look at temporal effects on price by charting home sales price over time and breaking down which months of the year generates the most homes sold.\n",
    "The first step in our Exploratory Data Analysis will be to get familiar with the data. This step includes:\n",
    "\n",
    "Understanding the dimensionality of your dataset\n",
    "Investigating what type of data it contains, and the data types used to store it\n",
    "Discovering how missing values are encoded, and how many there are\n",
    "Getting a feel for what information it does and doesnt contain\n",
    "Which powers have the highest chance of co-occuring in a hero (e.g. super strength and flight), and does this differ by gender?\n",
    "Is there a relationship between a hero's height and weight and their powerset?\n",
    "What is the distribution of skin colors amongst alien heroes?\n",
    "\n",
    "Model: We begin preprocessing of data by creating a heatmap of variable correlations to remove highly correlated variables. We also transform the data to be more Gaussian-like because we notice some skewness among variable distributions and scale the data to ensure a range between -1 and 1, with a zero-centered mean. Next, we create dummy variables for the categorical data. We create a train-test split and fit the initial model to the train set. After obtaining a basic accuracy metric, we go back and transofrm the target variable to obtain a higher accuracy metric. We also determine the best number of predictors to use in the model by selecting those with p-value < 0.05. Once we are satisfied with tuning, we fit the final model on the test set.\n",
    "\n",
    "Interpret: Building grade, zipcode, yr_renovated and waterfront_1.0 are all important predictors that contribute to the higher overall sales price of the home. While some of these variables may be out of our control (such as waterfront view), sellers can maximize other variables such as increasing home value by renovating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pandas, we import the data for Kings County Housing and look at a few basics like length, columns and the first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>10/13/2014</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2/25/2015</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2/18/2015</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  10/13/2014  221900.0         3       1.00         1180   \n",
       "1  6414100192   12/9/2014  538000.0         3       2.25         2570   \n",
       "2  5631500400   2/25/2015  180000.0         2       1.00          770   \n",
       "3  2487200875   12/9/2014  604000.0         4       3.00         1960   \n",
       "4  1954400510   2/18/2015  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0         NaN   0.0  ...      7        1180            0.0   \n",
       "1      7242     2.0         0.0   0.0  ...      7        2170          400.0   \n",
       "2     10000     1.0         0.0   0.0  ...      6         770            0.0   \n",
       "3      5000     1.0         0.0   0.0  ...      7        1050          910.0   \n",
       "4      8080     1.0         0.0   0.0  ...      8        1680            0.0   \n",
       "\n",
       "  yr_built  yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0     1955           0.0    98178  47.5112 -122.257           1340        5650  \n",
       "1     1951        1991.0    98125  47.7210 -122.319           1690        7639  \n",
       "2     1933           NaN    98028  47.7379 -122.233           2720        8062  \n",
       "3     1965           0.0    98136  47.5208 -122.393           1360        5000  \n",
       "4     1987           0.0    98074  47.6168 -122.045           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kc_house_data.csv') #Loads the dataframe using Pandas\n",
    "print(len(df)) #Prints the length of the dataframe\n",
    "df.head() #Uses a built in method common to all Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 21 columns):\n",
      "id               21597 non-null int64\n",
      "date             21597 non-null object\n",
      "price            21597 non-null float64\n",
      "bedrooms         21597 non-null int64\n",
      "bathrooms        21597 non-null float64\n",
      "sqft_living      21597 non-null int64\n",
      "sqft_lot         21597 non-null int64\n",
      "floors           21597 non-null float64\n",
      "waterfront       19221 non-null float64\n",
      "view             21534 non-null float64\n",
      "condition        21597 non-null int64\n",
      "grade            21597 non-null int64\n",
      "sqft_above       21597 non-null int64\n",
      "sqft_basement    21597 non-null object\n",
      "yr_built         21597 non-null int64\n",
      "yr_renovated     17755 non-null float64\n",
      "zipcode          21597 non-null int64\n",
      "lat              21597 non-null float64\n",
      "long             21597 non-null float64\n",
      "sqft_living15    21597 non-null int64\n",
      "sqft_lot15       21597 non-null int64\n",
      "dtypes: float64(8), int64(11), object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns represent those in the column_names.md file, we can immediately see some columns are miscattorgorized, which we will take care of during the data cleansing process. There are also some columns with null entries, which we will assess later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520 2014-10-13  221900.0         3       1.00         1180   \n",
       "1  6414100192 2014-12-09  538000.0         3       2.25         2570   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0         NaN   0.0  ...      7        1180            0.0   \n",
       "1      7242     2.0         0.0   0.0  ...      7        2170          400.0   \n",
       "\n",
       "  yr_built  yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0     1955           0.0    98178  47.5112 -122.257           1340        5650  \n",
       "1     1951        1991.0    98125  47.7210 -122.319           1690        7639  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We know immeditately that the date column should be a date\n",
    "df.date = pd.to_datetime(df.date)\n",
    "print(df.date.dtype)\n",
    "#Preview updated dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0 2014-10-13  221900.0         3       1.00         1180      5650     1.0   \n",
       "1 2014-12-09  538000.0         3       2.25         2570      7242     2.0   \n",
       "2 2015-02-25  180000.0         2       1.00          770     10000     1.0   \n",
       "3 2014-12-09  604000.0         4       3.00         1960      5000     1.0   \n",
       "4 2015-02-18  510000.0         3       2.00         1680      8080     1.0   \n",
       "\n",
       "   waterfront  view  condition  grade  sqft_above sqft_basement  yr_built  \\\n",
       "0         NaN   0.0          3      7        1180           0.0      1955   \n",
       "1         0.0   0.0          3      7        2170         400.0      1951   \n",
       "2         0.0   0.0          3      6         770           0.0      1933   \n",
       "3         0.0   0.0          5      7        1050         910.0      1965   \n",
       "4         0.0   0.0          3      8        1680           0.0      1987   \n",
       "\n",
       "   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0           0.0    98178  47.5112 -122.257           1340        5650  \n",
       "1        1991.0    98125  47.7210 -122.319           1690        7639  \n",
       "2           NaN    98028  47.7379 -122.233           2720        8062  \n",
       "3           0.0    98136  47.5208 -122.393           1360        5000  \n",
       "4           0.0    98074  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('id', axis=1) #we will not need id to model the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and yr_renovated shouldn't be a floating number, but we need to look at the NaN and 0.0 values first\n",
    "#df.yr_renovated = df.yr_renovated.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now, set the date as the index\n",
    "#df = df.set_index('date')\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>...</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>day_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1050</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0 2014-10-13  221900.0         3       1.00         1180      5650     1.0   \n",
       "1 2014-12-09  538000.0         3       2.25         2570      7242     2.0   \n",
       "2 2015-02-25  180000.0         2       1.00          770     10000     1.0   \n",
       "3 2014-12-09  604000.0         4       3.00         1960      5000     1.0   \n",
       "4 2015-02-18  510000.0         3       2.00         1680      8080     1.0   \n",
       "\n",
       "   waterfront  view  condition  ...  sqft_above  sqft_basement yr_built  \\\n",
       "0         NaN   0.0          3  ...        1180            0.0     1955   \n",
       "1         0.0   0.0          3  ...        2170          400.0     1951   \n",
       "2         0.0   0.0          3  ...         770            0.0     1933   \n",
       "3         0.0   0.0          5  ...        1050          910.0     1965   \n",
       "4         0.0   0.0          3  ...        1680            0.0     1987   \n",
       "\n",
       "   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \\\n",
       "0           0.0    98178  47.5112 -122.257           1340        5650   \n",
       "1        1991.0    98125  47.7210 -122.319           1690        7639   \n",
       "2           NaN    98028  47.7379 -122.233           2720        8062   \n",
       "3           0.0    98136  47.5208 -122.393           1360        5000   \n",
       "4           0.0    98074  47.6168 -122.045           1800        7503   \n",
       "\n",
       "    day_name  \n",
       "0     Monday  \n",
       "1    Tuesday  \n",
       "2  Wednesday  \n",
       "3    Tuesday  \n",
       "4  Wednesday  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding day of week may be interesting\n",
    "#dt stores all the built in datetime methods (only works for datetime columns)\n",
    "df['day_name'] = df.date.dt.day_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.159700e+04</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>2.159700e+04</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>19221.000000</td>\n",
       "      <td>21534.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>17755.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.402966e+05</td>\n",
       "      <td>3.373200</td>\n",
       "      <td>2.115826</td>\n",
       "      <td>2080.321850</td>\n",
       "      <td>1.509941e+04</td>\n",
       "      <td>1.494096</td>\n",
       "      <td>0.007596</td>\n",
       "      <td>0.233863</td>\n",
       "      <td>3.409825</td>\n",
       "      <td>7.657915</td>\n",
       "      <td>1788.596842</td>\n",
       "      <td>1970.999676</td>\n",
       "      <td>83.636778</td>\n",
       "      <td>98077.951845</td>\n",
       "      <td>47.560093</td>\n",
       "      <td>-122.213982</td>\n",
       "      <td>1986.620318</td>\n",
       "      <td>12758.283512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.673681e+05</td>\n",
       "      <td>0.926299</td>\n",
       "      <td>0.768984</td>\n",
       "      <td>918.106125</td>\n",
       "      <td>4.141264e+04</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.086825</td>\n",
       "      <td>0.765686</td>\n",
       "      <td>0.650546</td>\n",
       "      <td>1.173200</td>\n",
       "      <td>827.759761</td>\n",
       "      <td>29.375234</td>\n",
       "      <td>399.946414</td>\n",
       "      <td>53.513072</td>\n",
       "      <td>0.138552</td>\n",
       "      <td>0.140724</td>\n",
       "      <td>685.230472</td>\n",
       "      <td>27274.441950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.800000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.220000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.471100</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98065.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.231000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.450000e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068500e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.700000e+06</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price      bedrooms     bathrooms   sqft_living      sqft_lot  \\\n",
       "count  2.159700e+04  21597.000000  21597.000000  21597.000000  2.159700e+04   \n",
       "mean   5.402966e+05      3.373200      2.115826   2080.321850  1.509941e+04   \n",
       "std    3.673681e+05      0.926299      0.768984    918.106125  4.141264e+04   \n",
       "min    7.800000e+04      1.000000      0.500000    370.000000  5.200000e+02   \n",
       "25%    3.220000e+05      3.000000      1.750000   1430.000000  5.040000e+03   \n",
       "50%    4.500000e+05      3.000000      2.250000   1910.000000  7.618000e+03   \n",
       "75%    6.450000e+05      4.000000      2.500000   2550.000000  1.068500e+04   \n",
       "max    7.700000e+06     33.000000      8.000000  13540.000000  1.651359e+06   \n",
       "\n",
       "             floors    waterfront          view     condition         grade  \\\n",
       "count  21597.000000  19221.000000  21534.000000  21597.000000  21597.000000   \n",
       "mean       1.494096      0.007596      0.233863      3.409825      7.657915   \n",
       "std        0.539683      0.086825      0.765686      0.650546      1.173200   \n",
       "min        1.000000      0.000000      0.000000      1.000000      3.000000   \n",
       "25%        1.000000      0.000000      0.000000      3.000000      7.000000   \n",
       "50%        1.500000      0.000000      0.000000      3.000000      7.000000   \n",
       "75%        2.000000      0.000000      0.000000      4.000000      8.000000   \n",
       "max        3.500000      1.000000      4.000000      5.000000     13.000000   \n",
       "\n",
       "         sqft_above      yr_built  yr_renovated       zipcode           lat  \\\n",
       "count  21597.000000  21597.000000  17755.000000  21597.000000  21597.000000   \n",
       "mean    1788.596842   1970.999676     83.636778  98077.951845     47.560093   \n",
       "std      827.759761     29.375234    399.946414     53.513072      0.138552   \n",
       "min      370.000000   1900.000000      0.000000  98001.000000     47.155900   \n",
       "25%     1190.000000   1951.000000      0.000000  98033.000000     47.471100   \n",
       "50%     1560.000000   1975.000000      0.000000  98065.000000     47.571800   \n",
       "75%     2210.000000   1997.000000      0.000000  98118.000000     47.678000   \n",
       "max     9410.000000   2015.000000   2015.000000  98199.000000     47.777600   \n",
       "\n",
       "               long  sqft_living15     sqft_lot15  \n",
       "count  21597.000000   21597.000000   21597.000000  \n",
       "mean    -122.213982    1986.620318   12758.283512  \n",
       "std        0.140724     685.230472   27274.441950  \n",
       "min     -122.519000     399.000000     651.000000  \n",
       "25%     -122.328000    1490.000000    5100.000000  \n",
       "50%     -122.231000    1840.000000    7620.000000  \n",
       "75%     -122.125000    2360.000000   10083.000000  \n",
       "max     -121.315000    6210.000000  871200.000000  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <th>price</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>262000.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>520.0</td>\n",
       "      <td>12981.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98022.0</td>\n",
       "      <td>47.2082</td>\n",
       "      <td>-121.995</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>12233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">4</th>\n",
       "      <th>80000.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>430.0</td>\n",
       "      <td>5050.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98014.0</td>\n",
       "      <td>47.6499</td>\n",
       "      <td>-121.909</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>560.0</td>\n",
       "      <td>4120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98106.0</td>\n",
       "      <td>47.5335</td>\n",
       "      <td>-122.348</td>\n",
       "      <td>980.0</td>\n",
       "      <td>4120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>960.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98198.0</td>\n",
       "      <td>47.3864</td>\n",
       "      <td>-122.307</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>8120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>660.0</td>\n",
       "      <td>5240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98032.0</td>\n",
       "      <td>47.3881</td>\n",
       "      <td>-122.234</td>\n",
       "      <td>850.0</td>\n",
       "      <td>5080.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "grade price                                                          \n",
       "3     262000.0       1.0       0.75        520.0   12981.0     1.0   \n",
       "4     80000.0        1.0       0.75        430.0    5050.0     1.0   \n",
       "      90000.0        1.0       1.00        560.0    4120.0     1.0   \n",
       "      95000.0        2.0       1.00        960.0    7000.0     1.0   \n",
       "      100000.0       2.0       0.75        660.0    5240.0     1.0   \n",
       "\n",
       "                waterfront  view  condition  sqft_above  yr_built  \\\n",
       "grade price                                                         \n",
       "3     262000.0         0.0   0.0        5.0       520.0    1920.0   \n",
       "4     80000.0          NaN   0.0        2.0       430.0    1912.0   \n",
       "      90000.0          0.0   0.0        3.0       560.0    1947.0   \n",
       "      95000.0          0.0   0.0        3.0       960.0    1918.0   \n",
       "      100000.0         0.0   0.0        4.0       660.0    1912.0   \n",
       "\n",
       "                yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "grade price                                                              \n",
       "3     262000.0           0.0  98022.0  47.2082 -121.995         1340.0   \n",
       "4     80000.0            0.0  98014.0  47.6499 -121.909         1200.0   \n",
       "      90000.0            0.0  98106.0  47.5335 -122.348          980.0   \n",
       "      95000.0            0.0  98198.0  47.3864 -122.307         1850.0   \n",
       "      100000.0           0.0  98032.0  47.3881 -122.234          850.0   \n",
       "\n",
       "                sqft_lot15  \n",
       "grade price                 \n",
       "3     262000.0     12233.0  \n",
       "4     80000.0       7500.0  \n",
       "      90000.0       4120.0  \n",
       "      95000.0       8120.0  \n",
       "      100000.0      5080.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Groupby State and Gender and Race. Find the average values.\n",
    "df.groupby(['grade','price']).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'price')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['grade'])['price'].mean().sort_values().plot(kind='barh', figsize=(15,8))\n",
    "plt.title('mean price by grade')\n",
    "plt.xlabel(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     8974\n",
       "8     6065\n",
       "9     2615\n",
       "6     2038\n",
       "10    1134\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.grade.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                0\n",
       "price               0\n",
       "bedrooms            0\n",
       "bathrooms           0\n",
       "sqft_living         0\n",
       "sqft_lot            0\n",
       "floors              0\n",
       "waterfront       2376\n",
       "view               63\n",
       "condition           0\n",
       "grade               0\n",
       "sqft_above          0\n",
       "sqft_basement       0\n",
       "yr_built            0\n",
       "yr_renovated     3842\n",
       "zipcode             0\n",
       "lat                 0\n",
       "long                0\n",
       "sqft_living15       0\n",
       "sqft_lot15          0\n",
       "day_name            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date             False\n",
       "price            False\n",
       "bedrooms         False\n",
       "bathrooms        False\n",
       "sqft_living      False\n",
       "sqft_lot         False\n",
       "floors           False\n",
       "waterfront        True\n",
       "view              True\n",
       "condition        False\n",
       "grade            False\n",
       "sqft_above       False\n",
       "sqft_basement    False\n",
       "yr_built         False\n",
       "yr_renovated      True\n",
       "zipcode          False\n",
       "lat              False\n",
       "long             False\n",
       "sqft_living15    False\n",
       "sqft_lot15       False\n",
       "day_name         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Null waterfront Values: 0.11001527989998611\n",
      "Number of waterfront Values: 2\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of Null waterfront Values:', len(df[df.waterfront.isna()])/ len(df))\n",
    "print('Number of waterfront Values:', df.waterfront.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Null view Values: 0.0029170718155299346\n",
      "Number of view Values: 5\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of Null view Values:', len(df[df.view.isna()])/ len(df))\n",
    "print('Number of view Values:', df.view.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Null yr_renovated Values: 0.17789507802009538\n",
      "Number of yr_renovated Values: 70\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of Null yr_renovated Values:', len(df[df.yr_renovated.isna()])/ len(df))\n",
    "print('Number of yr_renovated Values:', df.yr_renovated.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for yr_renovated:\n",
      "[   0. 1991.   nan 2002. 2010. 1992. 2013. 1994. 1978. 2005. 2003. 1984.\n",
      " 1954. 2014. 2011. 1983. 1945. 1990. 1988. 1977. 1981. 1995. 2000. 1999.\n",
      " 1998. 1970. 1989. 2004. 1986. 2007. 1987. 2006. 1985. 2001. 1980. 1971.\n",
      " 1979. 1997. 1950. 1969. 1948. 2009. 2015. 1974. 2008. 1968. 2012. 1963.\n",
      " 1951. 1962. 1953. 1993. 1996. 1955. 1982. 1956. 1940. 1976. 1946. 1975.\n",
      " 1964. 1973. 1957. 1959. 1960. 1967. 1965. 1934. 1972. 1944. 1958.]\n",
      "\n",
      "\n",
      "Values for view:\n",
      "[ 0. nan  3.  4.  2.  1.]\n",
      "\n",
      "\n",
      "Values for waterfront:\n",
      "[nan  0.  1.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ['yr_renovated','view', 'waterfront']:\n",
    "    print('Values for {}:\\n{}\\n\\n'.format(col, df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yr_renovated'] = df['yr_renovated'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['view'] = df['view'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['waterfront'] = df['waterfront'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1        1991\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "15          0\n",
       "16          0\n",
       "17          0\n",
       "18          0\n",
       "19          0\n",
       "20          0\n",
       "21          0\n",
       "22          0\n",
       "23          0\n",
       "24          0\n",
       "25          0\n",
       "26          0\n",
       "27          0\n",
       "28          0\n",
       "29          0\n",
       "         ... \n",
       "21567       0\n",
       "21568       0\n",
       "21569       0\n",
       "21570       0\n",
       "21571       0\n",
       "21572       0\n",
       "21573       0\n",
       "21574       0\n",
       "21575       0\n",
       "21576       0\n",
       "21577       0\n",
       "21578       0\n",
       "21579       0\n",
       "21580       0\n",
       "21581       0\n",
       "21582       0\n",
       "21583       0\n",
       "21584       0\n",
       "21585       0\n",
       "21586       0\n",
       "21587       0\n",
       "21588       0\n",
       "21589       0\n",
       "21590       0\n",
       "21591       0\n",
       "21592       0\n",
       "21593       0\n",
       "21594       0\n",
       "21595       0\n",
       "21596       0\n",
       "Name: view, Length: 21597, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the month from the date string using a lambda function.\n",
    "#df.date.map(lambda x: x[3:5]).head()\n",
    "#What is the average number of words for a yelp review?\n",
    "#df.text.map(lambda x: len(x.split())).mean()\n",
    "#Create a new column for the number of words in the review.\n",
    "#df['Review_num_words'] = df.text.map(lambda x: len(x.split()))\n",
    "#df.head(2)\n",
    "#df['Review_length'] = df['Review_num_words'].map(lambda x: 'Short' if x < 50 else ('Medium' if x < 80 else 'Long'))\n",
    "#df.Review_length.value_counts(normalize=True)\n",
    "#conditionals df['text'].map(lambda x: 'Good' if any([word in x.lower() for word in ['awesome', 'love', 'good', 'great']]) else 'Bad').head()\n",
    "#fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10,10))\n",
    "#x = np.linspace(start=-10, stop=10, num=10*83)\n",
    "#for i in range(12):\n",
    "#    row = i//4\n",
    "#    col = i%4\n",
    "#    ax = axes[row, col]\n",
    "#    ax.scatter(x, x**i)\n",
    "#    ax.set_title('Plot of x^{}'.format(i))\n",
    "#plt.show()\n",
    "#grouping multiple df.groupby(['Sex', 'Pclass']).mean()\n",
    "#df.groupby(['Sex', 'Pclass'])['Survived'].mean()\n",
    "#concat big_df = pd.concat(to_concat)\n",
    "#fill with median dataframe['Fare'].fillna(df['Fare'].median())\n",
    "#age_mean = df.Age.mean()\n",
    "#age_median = df.Age.median()\n",
    "#df['Age'].plot(kind='hist', bins=80)\n",
    "#print(\"Mean Value for Age column: {}\".format(age_mean))\n",
    "#print(\"Median Value for Age column: {}\".format(age_median))\n",
    "\n",
    "#df.Pclass.value_counts(normalize=True)\n",
    "\n",
    "#randomly fill\n",
    "#Observation: account for 5% of the data\n",
    "#Method: randomly select a class acccording to current distribution\n",
    "#rel_prob = [.53, .22, .19]\n",
    "#prob = [i/sum(rel_prob) for i in rel_prob]\n",
    "#def impute_pclass(value):\n",
    "#    if value == '?':\n",
    "#        return np.random.choice(['3','1','2'], p=prob)\n",
    "#    else:\n",
    "#        return value\n",
    "#df.Pclass = df.Pclass.map(lambda x: impute_pclass(x))\n",
    "#df.Pclass.value_counts(normalize=True)\n",
    "\n",
    "#rename & Join\n",
    "#powers_df.rename(columns={'hero_names':'name'}, inplace=True)\n",
    "#powers_df  = powers_df.astype('str')\n",
    "#heroes_and_powers_df = powers_df.set_index('name').join(heroes_df.set_index('name'), how='inner')\n",
    "#heroes_and_powers_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#male_heroes_df = heroes_df[heroes_df['Gender'] == 'Male']\n",
    "#female_heroes_df = heroes_df[heroes_df['Gender'] == 'Female']\n",
    "\n",
    "#def show_distplot(dataframe, gender, column_name):\n",
    "#    plt.plot()\n",
    "#    dataframe[column_name].hist()\n",
    "#    plt.title(\"Distribution of {} for {} heroes\".format(column_name, gender))\n",
    "#    plt.xlabel(column_name)\n",
    "#    plt.ylabel(\"Probability Density\")\n",
    "#    plt.show()\n",
    "\n",
    "#show_distplot(heroes_and_powers_df, 'Male', 'Height')\n",
    "#print(\"Mean Height for male heroes: {}\".format(male_heroes_df.Height.mean()))\n",
    "#print(\"Median Height for male heroes: {}\".format(male_heroes_df.Height.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - remember to use markdown cells for comments as well!\n",
    "#df[df.sqft_basement == '?'].shape[0]\n",
    "##df.sqft_basement.value_counts()[:10]\n",
    "#print('Houses with no basement:', round( sum(df.sqft_basement == '0.0') / len(df), 2 ) * 100, '%' )\n",
    "#df.sqft_basement = df.sqft_basement.map(lambda x: '0' if x == '?' else x)\n",
    "#df.sqft_basement = df.sqft_basement.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['renovated'] = df.apply(lambda row: 0 if row.yr_renovated < 1 else 1, axis=1)\n",
    "#df.renovated = df.renovated.astype('category')\n",
    "#df.drop('yr_renovated', axis=1, inplace=True)\n",
    "#df.hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-b7c1060ed64f>, line 107)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-b7c1060ed64f>\"\u001b[0;36m, line \u001b[0;32m107\u001b[0m\n\u001b[0;31m    Normalize the biased list and calculate the new pmf\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#5mostcommon\n",
    "#def top_5_powers(dataframe):\n",
    "#    df = dataframe.drop(heroes_df.columns.values[1:], axis=1)\n",
    "#    columns = df.columns.values\n",
    "#    for col in columns:\n",
    "#        df[col] = df[col].map({\"True\": 1, \"False\": 0})\n",
    "        \n",
    "#    power_counts_dict = dict(df.sum())\n",
    "    \n",
    "#    return sorted(power_counts_dict.items(), key=lambda x: x[1], reverse=True)[:5] \n",
    "    \n",
    "#overall_top_5 = top_5_powers(heroes_and_powers_df)\n",
    "#marvel_df = heroes_and_powers_df[heroes_and_powers_df['Publisher'] == 'Marvel Comics']\n",
    "#dc_df = heroes_and_powers_df[heroes_and_powers_df['Publisher'] == 'DC Comics']\n",
    "#print(overall_top_5)\n",
    "#marvel_top_5 = top_5_powers(marvel_df)\n",
    "#print(marvel_top_5)\n",
    "#def top_5_bar_chart(top_5_list, publisher=None):\n",
    "#    marvel_powers = [i[0] for i in top_5_list]\n",
    "#    marvel_values = [i[1] for i in top_5_list]\n",
    "\n",
    "#    plt.clf()\n",
    "#    plt.figure(figsize=(10, 7))\n",
    "#    bar_positions = np.arange(len(marvel_powers))\n",
    "#    plt.bar(bar_positions, marvel_values)\n",
    "#    plt.xticks(bar_positions, marvel_powers)\n",
    "#    if publisher:\n",
    "#        plt.title(\"Top 5 Powers in {} Universe\".format(publisher))\n",
    "#    else:\n",
    "#        plt.title(\"Top 5 Powers in Superheroes Dataset\")\n",
    "#    plt.show()\n",
    "\n",
    "#display(top_5_bar_chart(overall_top_5))\n",
    "#display(top_5_bar_chart(dc_top_5, publisher=\"DC Comics\"))\n",
    "#top_5_bar_chart(marvel_top_5, publisher=\"Marvel Comics\")\n",
    "\n",
    "#1. Calculate the probability that at least 2 out of the first 3 appointments are with female students\n",
    "#First, select the first 3 appointment slots and check for \"F\".\n",
    "\n",
    "#first_3_F = sample_mf[:,:3] == \"F\"\n",
    "#print(first_3_F)\n",
    "#num_F = np.sum(first_3_F, axis=1)\n",
    "#print(num_F)\n",
    "#F_2plus = np.sum(num_F > 1)\n",
    "#print(F_2plus)\n",
    "#prob_F_2plus = F_2plus.sum()/sample_length\n",
    "#print(prob_F_2plus)\n",
    "#np.sum((sample_mf[:,4:]== ['M','M']).all(axis=1))/sample_length\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "# Create stems and leafs arrays to store the grades for all the marks in marks array, in the same order.\n",
    "stems = []\n",
    "leafs = []\n",
    "\n",
    "for mark in marks:\n",
    "    stem = mark //10\n",
    "    leaf = mark %10\n",
    "    stems.append(stem)\n",
    "    leafs.append(leaf)\n",
    "    \n",
    "# Create a stem and leaf plot including the above styling\n",
    "plt.figure(figsize=(12,8))\n",
    "# markerline, stemlines, baseline = \n",
    "\n",
    "plt.stem(stems, leafs, '-.', 'o' )\n",
    "plt.title('Stem and Leaf Plot for Student Marks', fontsize = 30 )\n",
    "plt.ylabel('Leafs', fontsize = 20)\n",
    "plt.xlabel('Stems', fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "plt.hist(marks)\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import collections\n",
    "sum_class = np.array([8, 8, 14, 4, 6, 12, 8, 3, 2]).sum()\n",
    "counter = collections.Counter(size_and_count)\n",
    "pmf = []\n",
    "sum_class = np.array(counter.values()).sum\n",
    "for key,val in counter.items():\n",
    "    pmf.append(round(val/65, 3))\n",
    "    \n",
    "sizes = list(counter.keys())\n",
    "sizes, pmf\n",
    "np.array(pmf).sum()\n",
    "x = list(counter.keys())\n",
    "y = pmf\n",
    "mu = 0\n",
    "for a,b in zip (x,y):\n",
    "    mu+=a*b\n",
    "mu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.stem(counter.keys(), pmf, '-', 'go', 'r-');\n",
    "plt.title (\"A Probability Mass Function\");\n",
    "\n",
    "biased = []\n",
    "for s, p in zip(sizes,pmf):\n",
    "    biased.append(s*p)\n",
    "\n",
    "biased_sum = np.array(biased).sum()\n",
    "biased, biased_sum\n",
    "Normalize the biased list and calculate the new pmf\n",
    "pmf2 = []\n",
    "for b in biased:\n",
    "    pmf2.append(round(b/biased_sum, 3))\n",
    "    \n",
    "sizes, pmf2\n",
    "\n",
    "mu_biased =0\n",
    "for a,b in zip (sizes,pmf2):\n",
    "    mu_biased += a*b\n",
    "mu_biased\n",
    "\n",
    "# Plot pmfs side by side\n",
    "new_figure = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "ax.stem(counter.keys(), pmf, '-', 'go', 'r-');\n",
    "ax2.stem(counter.keys(), pmf2, '-', 'ro', 'r-');\n",
    "\n",
    "ax.set_title (\"Probability Mass Function - Actual\");\n",
    "ax2.set_title (\"Probability Mass Function - Observed\");\n",
    "\n",
    "plt.show()\n",
    "a Box and Whiskers plot\n",
    "a histogram\n",
    "Non parametric Kernel Density Estimation plot\n",
    "Parametric distribution fit plot\n",
    "Provide options for customizations using keywords object specific _kws{}\n",
    ".. all in a single go.\n",
    "\n",
    "import scipy.stats as stats\n",
    "# Create two vertical subplots sharing 15% and 85% of plot space\n",
    "# sharex allows sharing of axes i.e. building multiple plots on same axes\n",
    "fig, (ax, ax2) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)}, figsize = (10,8) )\n",
    "​\n",
    "sns.distplot(data.Height, \n",
    "             hist=True, hist_kws={\n",
    "                                  \"linewidth\": 2,\n",
    "                                  \"edgecolor\" :'salmon',\n",
    "                                  \"alpha\": 0.4, \n",
    "                                  \"color\":  \"w\",\n",
    "                                  \"label\": \"Histogram\",\n",
    "                                  },\n",
    "             kde=True, kde_kws = {'linewidth': 3,\n",
    "                                  'color': \"blue\",\n",
    "                                  \"alpha\": 0.7,\n",
    "                                  'label':'Kernel Density Estimation Plot'\n",
    "                                 },\n",
    "             fit= stats.norm, fit_kws = {'color' : 'green',\n",
    "                                         'label' : 'parametric fit',\n",
    "                                         \"alpha\": 0.7,\n",
    "                                          'linewidth':3},\n",
    "             ax=ax2)\n",
    "ax2.set_title('Density Estimations')\n",
    "​\n",
    "sns.boxplot(x=data.Height, ax = ax,color = 'salmon')\n",
    "ax.set_title('Box and Whiskers Plot')\n",
    "ax2.set(ylim=(0, .08))\n",
    "plt.ylim(0,0.11)\n",
    "plt.legend()\n",
    "sns.distplot(data.Height,\n",
    "             hist_kws=dict(cumulative=True),\n",
    "             kde_kws=dict(cumulative=True));\n",
    "import numpy as np\n",
    "n, bins = np.histogram(data.Height, 20, density=1)\n",
    "n , bins\n",
    "# Initialize numpy arrays according to number of bins with zeros to store interpolated values\n",
    "pdfx = np.zeros(n.size)\n",
    "pdfy = np.zeros(n.size)\n",
    "\n",
    "# Interpolate through histogram bins \n",
    "# identify middle point between two neighbouring bins, in terms of x and y coords\n",
    "for k in range(n.size):\n",
    "    pdfx[k] = 0.5*(bins[k]+bins[k+1])\n",
    "    pdfy[k] = n[k]\n",
    "\n",
    "# plot the calculated curve\n",
    "plt.plot(pdfx, pdfy)\n",
    "\n",
    "Calculate the mean and standard deviation for weights and heights for male and female individually.\n",
    "Hint : Use your pandas dataframe subsetting skills like loc(), iloc() and groupby()\n",
    "\n",
    "data = pd.read_csv('weight-height.csv')\n",
    "male_df =  data.loc[data['Gender'] == 'Male']\n",
    "female_df =  data.loc[data['Gender'] == 'Female']\n",
    "\n",
    "print('Male Height mean:', male_df.Height.mean())\n",
    "print('Male Height sd:', male_df.Height.std())      \n",
    "\n",
    "print('Male Weight mean:', male_df.Weight.mean())\n",
    "print('Male Weight sd:' ,male_df.Weight.std())   \n",
    "\n",
    "print('Female Height mean:', female_df.Height.mean())\n",
    "print('Female Height sd:' ,female_df.Height.std())      \n",
    "\n",
    "print('Female Weight mean:', female_df.Weight.mean())\n",
    "print('Female Weight sd:' ,female_df.Weight.std())   \n",
    "\n",
    "binsize = 10\n",
    "male_df.Height.plot.hist(bins = binsize, normed = True,  alpha = 0.7, label =\"Male Height\");\n",
    "female_df.Height.plot.hist(bins = binsize, normed = True, alpha = 0.7, label = 'Female Height');\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Write a function density() that takes in a random variable and calculates the density function using np.hist and interpolation. The function should return two lists carrying x and y coordinates for plotting the density function\n",
    "def density(x):\n",
    "    \n",
    "    n, bins = np.histogram(x, 10, density=1)\n",
    "    # Initialize numpy arrays with zeros to store interpolated values\n",
    "    pdfx = np.zeros(n.size)\n",
    "    pdfy = np.zeros(n.size)\n",
    "\n",
    "    # Interpolate through histogram bins \n",
    "    # identify middle point between two neighbouring bins, in terms of x and y coords\n",
    "    for k in range(n.size):\n",
    "        pdfx[k] = 0.5*(bins[k]+bins[k+1])\n",
    "        pdfy[k] = n[k]\n",
    "\n",
    "    # plot the calculated curve\n",
    "    return pdfx, pdfy\n",
    "\n",
    "\n",
    "# Generate test data and test the function\n",
    "np.random.seed(5)\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 100)\n",
    "x,y = density(s)\n",
    "plt.plot(x,y, label = 'test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "male_df.Height.plot.hist(bins = binsize, normed = True,  alpha = 0.7, label =\"Male Height\");\n",
    "female_df.Height.plot.hist(bins = binsize, normed = True, alpha = 0.7, label = 'Female Height');\n",
    "plt.legend()\n",
    "x,y = density(male_df.Height)\n",
    "plt.plot(x,y)\n",
    "x,y = density(female_df.Height)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.distplot(male_df.Height)\n",
    "sns.distplot(female_df.Height)\n",
    "plt.title('Comparing Heights')\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "Normal distribution\n",
    "\n",
    "mu, sigma = 0.5, 0.1\n",
    "N = 1000\n",
    "s = np.random.normal(mu, sigma, N)\n",
    "sns.distplot(s);\n",
    "\n",
    "Get the bin positions and count for each bin\n",
    "Refer to official documentation to view input and output options for plt.hist()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create the bins and histogram\n",
    "count, bins, ignored = plt.hist(s, 20, density=True)\n",
    "\n",
    "# Calculate the normal Density function \n",
    "density = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "# Plot histogram along with the density function\n",
    "plt.hist(s, 20, normed=True)\n",
    "plt.plot(bins, density)\n",
    "plt.show()\n",
    "\n",
    "Visualize the distribution using seaborn and plot the KDE\n",
    "import seaborn as sns\n",
    "sns.distplot(s, bins=20, kde=True)\n",
    "\n",
    "standard normal \n",
    "import seaborn as sns\n",
    "mean1, sd1 = 5, 3 # dist 1 \n",
    "mean2, sd2 = 10,2 # dist 2 \n",
    "d1 = np.random.normal(mean1, sd1, 1000)\n",
    "d2 = np.random.normal(mean2, sd2, 1000)\n",
    "sns.distplot(d1);\n",
    "sns.distplot(d2);\n",
    "\n",
    "# Stardardizing and visualizing distributions\n",
    "\n",
    "sns.distplot([(x - d1.mean())/d1.std() for x in d1]);\n",
    "sns.distplot([(x - d2.mean())/d2.std() for x in d2]);\n",
    "\n",
    "sns.distplot(df)\n",
    "What does the 3-sigma rule say about yields of trees ?\n",
    "\n",
    "# Perform any calculations necessary here\n",
    "mean = df.mean()\n",
    "sd = df.std()\n",
    "mean,sd\n",
    "\n",
    "# Write your answer here \n",
    "\n",
    "# mean value is 42.4 and standard deviation is 6 (rounded off)\n",
    "# 68% of tree yields have weight between (42.4 - 6) 36.4 and (42.4 - 6) 48.4 pounds; \n",
    "# 95% between 30.4 and 54.4; \n",
    "# Almost all between 24.4 and 60.4 pounds\n",
    "\n",
    "# Calculate z\n",
    "z = (35 - mean)/sd\n",
    "z\n",
    "\n",
    "X = mean + z*sd\n",
    "X\n",
    "# Yield of this tree is 53.5 pounds. \n",
    "\n",
    "Convert each tree’s yield is converted to a Z-score so that “new” derived variable is “Z-score for weight”. The units are still the apple trees. For the data set of all Z-scores:\n",
    "\n",
    "What is the shape?\n",
    "The mean?\n",
    "The standard deviation?\n",
    "z_data = [(x - df['0'].mean())/df['0'].std() for x in df['0']]\n",
    "sns.distplot(z_data)\n",
    "mean = np.mean(np.array(z_data))\n",
    "sd = np.std((np.array(z_data)))\n",
    "print ('Mean:', round(mean,2))\n",
    "print ('SD:', round(sd,2))\n",
    "\n",
    "# Your observations\n",
    "# It is a standard normal distribution\n",
    "# Mean is 0 (it is a very small figure that rounds off to 0)\n",
    "# SD is 1\n",
    "# This is obvious because we standardised the whole distribution.,\n",
    "\n",
    "skewness \n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "Generate a random normal variable x_random in numpy with 10,000 values. Set the mean value to 0 and standard deviation to 2\n",
    "Plot a histogram for data, set bins to auto (default).\n",
    "Calculate the skewness and kurtosis for this data distribution using above functions.\n",
    "Record your observations about calculated values and shape of the data.\n",
    "x_random = np.random.normal(0, 2, 10000)\n",
    "plt.hist(x_random, bins='auto')\n",
    "print ('Skewness =', skew(x_random))\n",
    "print ('kurtosis =', kurtosis(x_random))\n",
    "\n",
    "x = np.linspace( -5, 5, 10000 )\n",
    "y = 1./(np.sqrt(2.*np.pi)) * np.exp( -.5*(x)**2  )  # normal distribution\n",
    "\n",
    "Plot a histogram for data (y), set bins to auto (default).\n",
    "Calculate the skewness and kurtosis for this data distribution using above functions.\n",
    "Record your observations about calculated values and shape of the data.\n",
    "plt.hist(y, bins='auto')\n",
    "print ('Skewness =', skew(y))\n",
    "print ('kurtosis =', kurtosis(y))\n",
    "\n",
    "# A high positive skewness is observed as there are more values on the left \n",
    "# side of distribution mean than those on right side\n",
    "\n",
    "# A negative kurtosis value indicates that the distribution has lighter tails \n",
    "# and a flatter peak than the normal distribution. \n",
    "\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "x_bar = 102 # sample mean \n",
    "n = 50 # number of students\n",
    "sigma = 16 # sd of population\n",
    "mu = 100 # Population mean \n",
    "\n",
    "z = (x_bar - mu)/(sigma/sqrt(n))\n",
    "z\n",
    "\n",
    "Let's try to plot this z value on a standard normal distribution to see what it means.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.fill_between(x=np.arange(-4,0.88,0.01),\n",
    "                 y1= stats.norm.pdf(np.arange(-4,0.88,0.01)) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35,\n",
    "                 label= 'Area below z-statistic'\n",
    "                 )\n",
    "​\n",
    "plt.fill_between(x=np.arange(0.88,4,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(0.88,4,0.01)) ,\n",
    "                 facecolor='blue',\n",
    "                 alpha=0.35, \n",
    "                 label= 'Area above z-statistic')\n",
    "plt.legend()\n",
    "plt.title ('z-statistic = 0.88')\n",
    "\n",
    "stats.norm.cdf(z)\n",
    "The percent of area under the normal curve from negative infinity to .88 z score is 81.2% (from z-table and calculations), meaning the average intelligence of this set of students is greater than 81.2% of the population. But we wanted it to be greater than 95% to prove our hypothesis to be significantly correct.\n",
    "\n",
    "And we get our p value probability by subtracting z value from 1 , as sum of probabilities in a normal distribution is always 1\n",
    "\n",
    "pval = 1 - stats.norm.cdf(z)\n",
    "pval\n",
    "pval = 1 - stats.norm.cdf(z)\n",
    "pval\n",
    "\n",
    "# State you null and alternative hypotheses\n",
    "\n",
    "\n",
    "# Ha : time to rent the car is greater than 60 seconds\n",
    "# Ho : time to rent a car is less than or equal to 60 sec\n",
    "# Your solution here\n",
    "\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "mu = 60\n",
    "sigma = 30\n",
    "n=36\n",
    "x_bar = 75\n",
    "z = (x_bar - mu)/(sigma/math.sqrt(n))\n",
    "p = 1 - stats.norm.cdf(z)\n",
    "\n",
    "p,z\n",
    "# Interpret the results in terms of p-value obtained\n",
    "\n",
    "# with p value less than 0.05 , we can reject the null hypothesis and say that time to rent a car\n",
    "# is significantly more than what company claims. \n",
    "\n",
    "# Give your solution here \n",
    "\n",
    "import numpy as np \n",
    "x = np.array([434, 694, 457, 534, 720, 400, 484, 478, 610, 641, 425, 636, 454,\n",
    "514, 563, 370, 499, 640, 501, 625, 612, 471, 598, 509, 531])\n",
    "x_bar = x.mean()\n",
    "n = len(x)\n",
    "mu = 500\n",
    "sigma = 100\n",
    "z = (x_bar - mu)/(sigma/math.sqrt(n))\n",
    "p = 1 - stats.norm.cdf(z)\n",
    "p,z\n",
    "\n",
    "# p = 0.03593031911292577, z = 1.8\n",
    "\n",
    "# Interpret the results in terms of p-value obtained\n",
    "\n",
    "# The p value is less than tha alpha so we can colculde that:\n",
    "# the training has a SIGNIFICANT effect on the SAT outcome. \n",
    "\n",
    "# Write a function to take in an iterable, calculate the mean and subtract the mean value\n",
    "# from each element , creating and returning a new list. \n",
    "\n",
    "def mean_normalize(var):\n",
    "\n",
    "    norm = [] # Vector for storing output values \n",
    "    n = 0     # a counter to identify the position of next element in vector\n",
    "    mean = np.mean(var)\n",
    "    \n",
    "    # for each element in the vector, subtract from mean and add the result to norm\n",
    "    for i in var:\n",
    "        diff = var[n] - mean\n",
    "        norm.append(diff)\n",
    "        n = n + 1\n",
    "    \n",
    "    return norm\n",
    "\n",
    "mean_normalize([1,2,3,4,5]), mean_normalize([11,22,33,44,55])\n",
    "# Visualize the height data distribution before and after mean normalization \n",
    "\n",
    "height = mean_normalize(data.height)\n",
    "import seaborn as sns\n",
    "sns.distplot(data.height)\n",
    "sns.distplot(height)\n",
    "\n",
    "# Write a function to calculate the dot product of two iterables \n",
    "\n",
    "def dot_product(x,y):\n",
    "    n = 0  # a counter pointing to the current element of vector(s)\n",
    "    prod_vec = [] # Initliaze an empty list to store the results \n",
    "    \n",
    "    # For all elements in the vectors, multiply and save results in prod_vec\n",
    "    for i in range(len(x)):\n",
    "        prod = x[i]* y[i]\n",
    "        prod_vec.append(prod)\n",
    "        n += 1\n",
    "        \n",
    "    dot_prod = np.sum(prod_vec)\n",
    "    return dot_prod\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "\n",
    "dot_product(a,b)\n",
    "\n",
    "#  32  calculated as (1*4 + 2*5 + 3*6)\n",
    "\n",
    "# Calculate covariance using functions above\n",
    "\n",
    "def covariance(var1, var2):\n",
    "\n",
    "    # Formula for covariance is:\n",
    "    # [Sum (x_i - X)(y_i - Y)] / N-1 \n",
    "    \n",
    "    # Sanity Check : Check to see if both vectors are of same length\n",
    "    # Exit the function if variables have different lengths\n",
    "\n",
    "    if len(var1) != len(var2):\n",
    "        return None \n",
    "    else: \n",
    "       \n",
    "        # Mean normalize both variables \n",
    "        x = mean_normalize(var1)\n",
    "        y = mean_normalize(var2)\n",
    "        \n",
    "        # Take the dot product of mean normalized variables\n",
    "        result = dot_product(x,y)\n",
    "\n",
    "        # divide the dot product by n-1    \n",
    "        return result /((len(var1)) -1)\n",
    "\n",
    "covariance(data['height'], data['Weight'])\n",
    "\n",
    "# 144.75789473684208\n",
    "\n",
    "Let's verify our results with pandas built in dataFrame.cov() method.\n",
    "\n",
    "data.cov()\n",
    "\n",
    "# Plot a scatter graph between height and weight to visually inspect the relationship \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data.height, data.Weight)\n",
    "\n",
    "# Calculate Correlation between two variables using formula above\n",
    "import math\n",
    "def correlation(var1,var2):\n",
    "    \n",
    "    if len(var1) != len(var2):\n",
    "        return None \n",
    "    else: \n",
    "       \n",
    "        mean_norm_var1 = mean_normalize(var1)\n",
    "        mean_norm_var2 = mean_normalize(var2)\n",
    "        \n",
    "        # Try the numpy way for calculating doc product\n",
    "        var1_dot_var2 = [a * b for a, b in list(zip(mean_norm_var1, mean_norm_var2))]\n",
    "        \n",
    "        var1_squared = [i * i for i in mean_norm_var1]\n",
    "        var2_squared = [i * i for i in mean_norm_var2]\n",
    "        \n",
    "        return np.round(sum(var1_dot_var2) / math.sqrt(sum(var1_squared) * sum(var2_squared)), 2)\n",
    "\n",
    "correlation(data['height'], data['Weight'])\n",
    "\n",
    "data.corr()\n",
    "\n",
    "linear reg\n",
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "# Initialize vectors X and Y with given values and create a scatter plot\n",
    "X = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n",
    "Y = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X,Y)\n",
    "plt.show()\n",
    "\n",
    "# Your observations about relationship in X and Y \n",
    "\n",
    "# The relationship is very linear but not perfectly linear\n",
    "# THe best fit line should be able to explain this relationship with very low error\n",
    "# Write the function to calculate slope as: \n",
    "# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\n",
    "def calc_slope(xs,ys):\n",
    "    \n",
    "    # Use the slope formula above and calculate the slope\n",
    "    m = (((np.mean(xs)*np.mean(ys)) - np.mean(xs*ys)) /\n",
    "         ((np.mean(xs)**2) - np.mean(xs*xs)))\n",
    "    \n",
    "    return m\n",
    "\n",
    "calc_slope(X,Y)\n",
    "\n",
    "Write a function best_fit() that takes in X and Y, calculates the slope using above above and intercept using the formula. The function should return slope and intercept values.\n",
    "\n",
    "def best_fit(xs,ys):\n",
    "    \n",
    "    # use the slope function with intercept formula to return calculate slop and intercept from data points\n",
    "    m = calc_slope(xs,ys)\n",
    "    b = np.mean(ys) - m*np.mean(xs)\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "m, b = best_fit(X,Y)\n",
    "m,b\n",
    "# (0.5393518518518512, 6.379629629629633)\n",
    "\n",
    "Write a function reg_line() that takes in slope, intercept and X vector and calculates the regression line using Y= mX+b for each point in X\n",
    "\n",
    "def reg_line (m, b, xs):\n",
    "    \n",
    "    return [(m*x)+b for x in xs]\n",
    "\n",
    "regression_line = reg_line(m,b,X)\n",
    "\n",
    "plt.scatter(X,Y,color='#003F72', label=\"Data points\")\n",
    "plt.plot(X, regression_line, label= \"Regression Line\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# y = 6.37 + 0.53x\n",
    "\n",
    "# The line crosses y-axis at 6.37 (shown in the graph) - intercept\n",
    "# The slope of line is 0.53 - a slope 0 would a horizontal line , and slope = 1 would be a vertical one\n",
    "# Our slope creates an angle roughly around 45 degree between x and y . \n",
    "\n",
    "Let's try to find a y prediction for a new value of x = 7 and unknown y, and plot the new prediction with existing data\n",
    "\n",
    "x_new = 7\n",
    "y_predicted = (m*x_new)+b\n",
    "y_predicted\n",
    "\n",
    "# 10.155092592592592\n",
    "plt.scatter(X,Y,color='#000F72',label='data')\n",
    "plt.plot(X, regression_line, color='#880000', label='regression line')\n",
    "plt.scatter(x_new,y_predicted,color='r',label='Prediction: '+ str(np.round(y_predicted,1)))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# Calculate sum of squared errors between regression and mean line \n",
    "import numpy as np\n",
    "\n",
    "def sq_err(ys_a, ys_b):\n",
    "        \n",
    "    squarred_error = 0\n",
    "    for x in range(0, len(ys_a)):\n",
    "        squarred_error += (ys_a[x] - ys_b[x]) ** 2\n",
    "    return squarred_error\n",
    "\n",
    "# Check the output with some toy data\n",
    "Y_a = np.array([1,3,5,7])\n",
    "Y_b = np.array([1,4,5,8])\n",
    "\n",
    "sq_err(Y_a, Y_b)\n",
    "\n",
    "# Check the output with some toy data\n",
    "Y_a = np.array([1,3,5,7])\n",
    "Y_b = np.array([1,4,5,8])\n",
    "\n",
    "sq_err(Y_a, Y_b)\n",
    "\n",
    "# 2\n",
    "Calculate the mean of ys_real\n",
    "Calculate SSE using sq_err()\n",
    "Calculate SST using sq_err()\n",
    "Calculate R-squared from above values using given formula.\n",
    "# Calculate Y_mean , squared error for regression and mean line , and calculate r-squared\n",
    "\n",
    "def r_squared(ys_real, ys_predicted):\n",
    "    \n",
    "    # calculate the numerator\n",
    "    num = sq_err(ys_real, ys_predicted)\n",
    "    # calculate the denominator\n",
    "    denom = 0\n",
    "    for x in ys_real:\n",
    "        denom += (x - ys_real.mean()) ** 2\n",
    "    return 1 - (num / denom)\n",
    "\n",
    "# Check the output with some toy data\n",
    "Y_real = np.array([1,3,5,7])\n",
    "Y_pred = np.array([1,5,5,10])\n",
    "\n",
    "r_squared(Y_real, Y_pred)\n",
    "\n",
    "A very low value , but it was not from some real data. So now we have quite a few functions for calculating slope, intercept, bestfit line, plotting and calculating R-squared. In the next lab we'll put these all together to run a complete regression experiment.\n",
    "\n",
    "# Combine all the functions created so far to run a complete regression experiment. \n",
    "# Produce an output similar to the one shown below. \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "def calc_slope(xs,ys):\n",
    "\n",
    "    m = (((np.mean(xs)*np.mean(ys)) - np.mean(xs*ys)) /\n",
    "         ((np.mean(xs)**2) - np.mean(xs*xs)))\n",
    "    \n",
    "    return m\n",
    "\n",
    "def best_fit(xs,ys):\n",
    "\n",
    "    m = calc_slope(xs,ys)\n",
    "    b = np.mean(ys) - m*np.mean(xs)\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "def reg_line (m, b, X):\n",
    "    \n",
    "    return [(m*x)+b for x in X]\n",
    "\n",
    "def sum_sq_err(ys_real,ys_predicted):\n",
    "\n",
    "    sse =  sum((ys_predicted - ys_real) * (ys_predicted - ys_real))\n",
    "    \n",
    "    return sse\n",
    "\n",
    "def r_squared(ys_real, ys_predicted):\n",
    "    \n",
    "    # Calculate Y_mean , squared error for regression and mean line , and calculate r-squared\n",
    "    y_mean = [np.mean(ys_real) for y in ys_real]\n",
    "\n",
    "    sq_err_reg= sum_sq_err(ys_real, ys_predicted)\n",
    "    sq_err_y_mean = sum_sq_err(ys_real, y_mean)\n",
    "    \n",
    "    # Calculate r-squared \n",
    "    r_sq =  1 - (sq_err_reg/sq_err_y_mean)\n",
    "    \n",
    "    return r_sq\n",
    "\n",
    "def plot_reg(X,Y,Y_pred):\n",
    "    plt.scatter(X,Y,color='#003F72',label='data')\n",
    "    plt.plot(X, Y_pred, label='regression line')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "X = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float64)\n",
    "Y = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n",
    "\n",
    "m, b = best_fit(X,Y)\n",
    "Y_pred = reg_line(m, b, X)\n",
    "r_squared = r_squared(Y,Y_pred)\n",
    "\n",
    "print ('Basic Regression Diagnostics')\n",
    "print ('----------------------------')\n",
    "print ('Slope:', round(m,2))\n",
    "print ('Y-Intercept:', round(b,2))\n",
    "print ('R-Squared:', round(r_squared,2))\n",
    "print ('----------------------------')\n",
    "print ('Model: Y =',round(m,2),'* X +', round(b,2))\n",
    "\n",
    "plot_reg(X,Y,Y_pred)\n",
    "       \n",
    "# Basic Regression Diagnostics\n",
    "# ----------------------------\n",
    "# Slope: 0.56\n",
    "# Y-Intercept: 6.33\n",
    "# R-Squared: 0.97\n",
    "# ----------------------------\n",
    "# Model: Y = 0.56 * X + 6.33\n",
    "\n",
    "Predict and plot the value of y using regression line above for a new value of x = 4.5.\n",
    "\n",
    "x_new = 4.5\n",
    "y_new = (m*x_new)+b\n",
    "y_new\n",
    "\n",
    "plt.scatter(X,Y,color='#000F72',label='data')\n",
    "plt.plot(X, Y_pred, color='#880000', label='regression line')\n",
    "plt.scatter(x_new,y_new,color='r',label='Prediction: '+ str(np.round(y_new,1)))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "ols\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "df = pd.read_csv('heightWeight.csv')\n",
    "df.plot.kde()\n",
    "plt.title(\"Normality check\")\n",
    "plt.show()\n",
    "plt.scatter(df.height, df.Weight)\n",
    "plt.title(\"Linearity check\")\n",
    "\n",
    "f = 'Weight~height'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()\n",
    "Wow , thats a lot of information. statsmodels performs a ton of tests and calculates measures to identify goodness of fit.\n",
    "\n",
    "We can find our R-squared which in this case is 0.95 i.e. very highly related.\n",
    "We can also look at the co-efficients of the model for slope and intercept\n",
    "Kurtosis and Skew values are shown on the terms we described earlier\n",
    "A lot of significance testing\n",
    "Here is a brief description of these measures:\n",
    "\n",
    "Dep. Variable: Singular. Which variable is the point of interest of the model\n",
    "Model: Technique used, abbreviated version of Method (see methods for more).\n",
    "Method: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. AKA Mean Square Error [MSE].\n",
    "No. Observations: The number of observations used by the model. Size of training data.\n",
    "Degrees of Freedom Residuals: Degrees of freedom of the residuals. Number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. Internal mechanism to ensures that there are enough observations to parameters.\n",
    "Degrees of Freedom Model: Number of parameters in the model (not including the constant/intercept term if present)\n",
    "Covariance Type: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers.\n",
    "The right part of the first table shows the goodness of fit\n",
    "\n",
    "R-squared: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. In English, translates in the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, that which the model and predictors fail to grasp.\n",
    "Adj. R-squared: Version of the R-Squared that penalizes additional independent variables. Similar to the concept of flexibility in the Bias-Variance tradeoff where high flexibility reduces bias, but puts the model at risk of high variance; the magnitude of effect a single observation can have on the model outcome. This lowers model robustness and model generalization.\n",
    "F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "Prob (F-statistic) or P-Value: The probability that a sample like this would yield the above statistic, and whether the models verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "Log-likelihood: The log of the likelihood function.\n",
    "AIC: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "BIC: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n",
    "Second Table: Coefficient Reports\n",
    "\n",
    "coef: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "std err: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "t: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "P > |t|: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "[95.0% Conf. Interval]: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n",
    "Third Table: Residuals, Autocorrelation, and Multicollinearity\n",
    "\n",
    "Skewness: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "Kurtosis: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakyness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "Omnibus D’Angostino’s test: It provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "Prob(Omnibus): The above statistic turned into a probability\n",
    "Jarque-Bera: A different test of the skewness and kurtosis\n",
    "Prob (JB): The above statistic turned into a probability\n",
    "Durbin-Watson: A test for the presence of autocorrelation (that the errors are not independent.) Often important in time-series analysis\n",
    "Cond. No: A test for multicollinearity (if in a fit with multiple parameters, the parameters are related with each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-f975b8822568>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-f975b8822568>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    plot some visualizations to check for regression assumptions in error terms. We shall use sm.graphics.plot_regress_exog() for some built in visualization capabilities of statsmodels. here is how we do it.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plot some visualizations to check for regression assumptions in error terms. We shall use sm.graphics.plot_regress_exog() for some built in visualization capabilities of statsmodels. here is how we do it.\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"height\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "import scipy.stats as stats\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()\n",
    "\n",
    "# Load necessary libraries and import the data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "\n",
    "# Check the columns and first few rows\n",
    "data.head()\n",
    "\n",
    "# Get the 5-point statistics for data \n",
    "data.describe()\n",
    "\n",
    "In every record, we have three predictors showing the advertising budget spent on TV, newspaper and radio and a target variable (sales). The target variable shows the sales figure for each marketing campaign along with money spent on all three channels. \n",
    "\n",
    "Looking at means for predictors, most budget is spent on TV marketing , and least on radio.\n",
    "\n",
    "# For all the variables, check if they hold normality assumption\n",
    "for column in data:\n",
    "    data[column].plot.hist(normed=True, label = column+' histogram')\n",
    "    data[column].plot.kde(label =column+' kde')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Record your observations on normality here \n",
    "No variable is \"perfectly\" normal, but these do tend to follow an overall normal pattern. We see major skew in the newspaper predictor which could be problematic towards analysis. \n",
    "\n",
    "TV and radio are still pretty symmetrical distributions and can be used as predictors\n",
    "\n",
    "The target variable \"sales\" is normally distributed with just a gentle skew\n",
    "\n",
    "# visualize the relationship between the preditors and the target using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, figsize=(18, 6))\n",
    "for idx, channel in enumerate(['TV', 'radio', 'newspaper']):\n",
    "    data.plot(kind='scatter', x=channel, y='sales', ax=axs[idx], label=channel)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "TV seems to be a good feature due to highly linear relationship with sales\n",
    "radio shows a linear pattern as well but there a higher level of variance in there than TV\n",
    "newspaper is worse, there is too much variance along y-axis and theres no clear linear relationship between newspaper and sales\n",
    "Step 4: Run a simple regression in statsmodels with TV as a predictor\n",
    "# import libraries\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# build the formula \n",
    "f = 'sales~TV'\n",
    "# create a fitted model in one line\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "\n",
    "Step 5: Get regression diagnostics summary\n",
    "model.summary()\n",
    "Record your observations on \"Goodness of fit\"\n",
    "R-squared value is 0.61 i.e. 61% of variance in the target variable can be explained using the spendings on TV. \n",
    "\n",
    "The Intercept: A \"unit\" increase in TV spending is associated with a 0.0475 \"unit\" increase in Sales. OR An additional 1,000 spent on TV is associated with an increase in sales of 47.5 \n",
    "\n",
    "Step 6: Draw a prediction line with data points omn a scatter plot for X (TV) and Y (Sales)\n",
    "Hint: We can use model.predict() functions to predict the start and end point of of regression line for the minimum and maximum values in the 'TV' variable.\n",
    "\n",
    "# create a DataFrame with the minimum and maximum values of TV\n",
    "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\n",
    "print(X_new.head())\n",
    "\n",
    "# make predictions for those x values and store them\n",
    "preds = model.predict(X_new)\n",
    "print (preds)\n",
    "\n",
    "# first, plot the observed data and the least squares line\n",
    "data.plot(kind='scatter', x='TV', y='sales')\n",
    "plt.plot(X_new, preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "Step 7: Visualize the error term for variance and heteroscedasticity\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"TV\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "From the first and second plot in the first row, we see that the variance is creating a cone-shape which is a sign of heteroscedasticity. i.e. the residuals are not normally distributed . This breaks the assumption. \n",
    "\n",
    "f = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "X_new = pd.DataFrame({'radio': [data.radio.min(), data.radio.max()]});\n",
    "preds = model.predict(X_new)\n",
    "data.plot(kind='scatter', x='radio', y='sales');\n",
    "plt.plot(X_new, preds, c='red', linewidth=2);\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"radio\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "As a predictor, radio performs worse than TV. \n",
    "\n",
    "It has higher amount of skewness and kurtosis than TV\n",
    "\n",
    "A very low R_squared explaining only 33% of variance in the target variable\n",
    "\n",
    "A \"unit\" increase in radio spending is associated with a 0.2025 \"unit\" increase in Sales. OR An additional 1,000 spent on TV is associated with an increase in sales of 20.02\n",
    "\n",
    "There is obvious heteroscedasticity as with the case of TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
